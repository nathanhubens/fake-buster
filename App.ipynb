{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c368f26",
   "metadata": {},
   "source": [
    "# Welcome to DeepFake-Buster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b70bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from fastai.vision.all import *\n",
    "from face_detection.BlazeFaceDetector import BlazeFaceDetector\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import imageio\n",
    "from custom import *\n",
    "import PIL\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsTransform(DisplayedTransform):\n",
    "    split_idx,order=0,2\n",
    "    def __init__(self, train_aug): store_attr()\n",
    "    \n",
    "    def encodes(self, img: PILImage):\n",
    "        aug_img = self.train_aug(image=np.array(img))['image']\n",
    "        return PILImage.create(aug_img)\n",
    "\n",
    "def get_item_tfms(size, blur_limit, var_limit, quality_lower, quality_upper, num_holes, hole_size):  \n",
    "    alb = A.Compose([\n",
    "            A.MotionBlur(blur_limit=blur_limit, p=0.3),\n",
    "            A.GaussNoise(var_limit=var_limit, p=0.3),\n",
    "            A.JpegCompression(quality_lower=quality_lower, quality_upper=quality_upper, p=0.3),\n",
    "            A.Cutout(num_holes=num_holes, max_h_size=hole_size, max_w_size=hole_size, p=0.3)\n",
    "])\n",
    "\n",
    "    return [Resize(size), AlbumentationsTransform(alb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e13473",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_learn = load_learner('dw_xrn18_sa_se_mixup_prog_prune_fp16_KD_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366245cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = BlazeFaceDetector(weights='face_detection/blazeface.pth', anchors='face_detection/anchors.npy', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_frames_sample(vid, n_frames=16):\n",
    "    vid = imageio.get_reader(io.BytesIO(vid), 'ffmpeg')\n",
    "    sample = np.linspace(0, int(vid.count_frames()) - 1 , min(n_frames, vid.count_frames())).astype(int)\n",
    "    list_frames = list(vid.iter_data())\n",
    "    return [list_frames[s] for s in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf310fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_frames(frames):\n",
    "    \n",
    "    detections = []\n",
    "    for frame in frames:\n",
    "        detection = detector.detect(frame)\n",
    "        if len(detection)>0: detections.append(detection)\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd342063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_faces(frame, detections):\n",
    "    \n",
    "    faces = []\n",
    "    \n",
    "    for detection in detections:\n",
    "        xmin = max(0, int(detection[0])) # Don't try to crop less than 0\n",
    "        ymin = max(0, int(detection[1])) # Don't try to crop less than 0\n",
    "        xmax = min(frame.shape[1], int(detection[2]))\n",
    "        ymax = min(frame.shape[0], int(detection[3]))\n",
    "        \n",
    "        face = frame[ymin:ymax, xmin:xmax]\n",
    "        \n",
    "        faces.append(face)\n",
    "    \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(faces):\n",
    "    return np.mean([detect_learn.predict(f)[2][0] for f in faces]) # returns the \"realness\" of the extracted face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5cca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload = widgets.FileUpload()\n",
    "btn_run = widgets.Button(description='Get Prediction')\n",
    "btn_att = widgets.Button(description='Show Attention')\n",
    "out_pl = widgets.Output()\n",
    "lbl_pred = widgets.Label()\n",
    "\n",
    "label = {0:'TRUE', 1:'FAKE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deae20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_children(change):\n",
    "    for o in change['owner'].children:\n",
    "        if not o.layout.flex: o.layout.flex = '0 0 auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carousel(children=(), **layout):\n",
    "    \"A horizontally scrolling carousel\"\n",
    "    def_layout = dict(overflow='scroll hidden', flex_flow='row', display='flex')\n",
    "    res = Box([], layout=merge(def_layout, layout))\n",
    "    res.observe(_update_children, names='children')\n",
    "    res.children = children\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def widget(im, *args, **layout):\n",
    "    \"Convert anything that can be `display`ed by IPython into a widget\"\n",
    "    o = Output(layout=merge(*args, layout))\n",
    "    with o: display(im)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click(change):\n",
    "    global faces\n",
    "    frames = read_frames_sample(btn_upload.data[-1])\n",
    "    frames_detections = predict_all_frames(frames)\n",
    "    faces = [np.array(get_cropped_faces(frames[i], frames_detections[i])).squeeze() for i in range(len(frames_detections))]\n",
    "    widg = carousel(width='100%')\n",
    "    ims = [PILImage.create(f).to_thumb(256, 256).convert('RGBA') for f in faces]\n",
    "    widg.children = [widget(im) for im in ims]\n",
    "    out_pl.clear_output()\n",
    "    with out_pl: display(widg)\n",
    "    \n",
    "\n",
    "btn_upload.observe(on_click, names=['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ca0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hook():\n",
    "    def __init__(self):\n",
    "        self.stored = []\n",
    "    def hook_func(self, m, i, o): self.stored.append(o.detach().clone())\n",
    "\n",
    "def on_click(change):\n",
    "    global hook_output\n",
    "    hook_output = Hook()\n",
    "    hook = detect_learn.model[4][1].convpath[4].conv.register_forward_hook(hook_output.hook_func)\n",
    "    pred = get_preds(faces)\n",
    "    pred_label = label[pred>0.5]\n",
    "    prob = 1-pred if pred<0.5 else pred\n",
    "    lbl_pred.value = f'I\\'m {100*prob:.02f}% sure that this image is {pred_label} !'\n",
    "\n",
    "btn_run.on_click(on_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a3a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(hm):\n",
    "    return (hm-hm.min())/(hm.max()-hm.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2788731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click(change):\n",
    "\n",
    "    blends = []\n",
    "    widg = carousel(width='100%')\n",
    "    hms = [(1-norm(output[0].mean(0)).view(64,64)) for output in hook_output.stored]\n",
    "    ims = detect_learn.dls.test_dl(faces)\n",
    "\n",
    "    imss = np.array(next(iter(ims))[0])\n",
    "\n",
    "\n",
    "    for hm, im, in zip(hms, imss):\n",
    "        fg = PIL.Image.fromarray(np.uint8(cm.magma(hm)*255))\n",
    "        bg = PIL.Image.fromarray(np.uint8(im.transpose(1,2,0)*255))\n",
    "        fg = fg.resize(bg.size, PIL.Image.BILINEAR)\n",
    "        bg = bg.convert('RGBA')\n",
    "        blends.append(PIL.Image.blend(bg, fg, alpha=0.5))\n",
    "\n",
    "\n",
    "    out_pl.clear_output()\n",
    "    widg.children = [widget(blend) for blend in blends]\n",
    "    with out_pl: display(widg)\n",
    "    \n",
    "\n",
    "btn_att.on_click(on_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720fa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6470f8d356544219bc51ffa454130060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Select your image!'), FileUpload(value={}, description='Upload'), Button(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1858649/3941008349.py:14: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  fg = fg.resize(bg.size, PIL.Image.BILINEAR)\n"
     ]
    }
   ],
   "source": [
    "display(VBox([widgets.Label('Select your image!'), btn_upload, btn_run, btn_att,lbl_pred, out_pl]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc1cc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
